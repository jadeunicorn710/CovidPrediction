{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#\n",
    "# Perform logistic regression model analysis using sklearn and keras (tensorflow) frameworks\n",
    "# and perform model analysis \n",
    "# \n",
    "# Note: This notebook define the model and class weights to help the model learn from \n",
    "#       the imbalanced data\n",
    "#\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Set parameters for LR\n",
    "#\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 400 \n",
    "LEARNING_RATE = 0.01  # SGD default value: 0.01\n",
    "MOMENTUM = 0.01   # SGD default value: 0.01\n",
    "\n",
    "FILENAME = \"../visualization/data/hospitalization_cleaned.csv\"\n",
    "TARGET = \"../visualization/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical \n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\n",
    "\n",
    "import time\n",
    "from time import time\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color for graph\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Data preprocessing\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "\n",
    "file = FILENAME\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify label and predictors\n",
    "\n",
    "label = ['patient_type']\n",
    "predictor = [\n",
    "    'age',  # age must be the first column for data pre-processing\n",
    "    'sex',\n",
    "    'pneumonia',\n",
    "    'diabetes',\n",
    "    'copd',\n",
    "    'asthma',\n",
    "    'inmsupr',\n",
    "    'hypertension',\n",
    "    'other_disease',\n",
    "    'cardiovascular',\n",
    "    'obesity',\n",
    "    'renal_chronic',\n",
    "    'tobacco'\n",
    "]\n",
    "\n",
    "label, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the dataset\n",
    "\n",
    "print(\"Dataframe shape: {}\".format(df[predictor].shape))\n",
    "df[predictor][0:5] # only 5 rows. You can also use either df[predictor].head() or df[predictor].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review imbalanced class column (label)\n",
    "\n",
    "neg, pos = np.bincount(df['patient_type'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numpy n-dimentional array (tensor) from the dataset (pandas' dataframe object)\n",
    "\n",
    "x = df[predictor].values\n",
    "y = df[label].values\n",
    "\n",
    "# Create train, test and validation datsets (in numpy's ndim-array format) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15)\n",
    "\n",
    "print(\"train shape: [features={}, label={}] \\ntest shape: [features={}, label={}] \\nvalidation shape: [features={}, label={}]\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset using standardscaler (mean: 0, std: 1) for numberic columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess train_data\n",
    "\n",
    "scaler_age = StandardScaler().fit(x_train[0:, 0:1])\n",
    "\n",
    "x_train_age = scaler_age.transform(x_train[0:, 0:1])\n",
    "x_train_remaining = x_train[0:, 1:]\n",
    "\n",
    "x_train_encoded = np.concatenate((x_train_age, x_train_remaining), axis=1)\n",
    "\n",
    "print(\"age column mean: {}, std: {}\".format(scaler_age.mean_, scaler_age.scale_))\n",
    "print(\"x_train encoded shape: {}\".format(x_train_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test_data\n",
    "\n",
    "x_test_age = scaler_age.transform(x_test[0:, 0:1])\n",
    "x_test_remaining = x_test[0:, 1:]\n",
    "\n",
    "x_test_encoded = np.concatenate((x_test_age, x_test_remaining), axis=1)\n",
    "print(\"x_test encoded shape: {}\".format(x_test_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess val_data\n",
    "\n",
    "x_val_age = scaler_age.transform(x_val[0:, 0:1])\n",
    "x_val_remaining = x_val[0:, 1:]\n",
    "\n",
    "x_val_encoded = np.concatenate((x_val_age, x_val_remaining), axis=1)\n",
    "print(\"x_val encoded shape: {}\".format(x_val_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Build ml models\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for ml models\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define various ml models e.g., standard neural network and logistic regression\n",
    "\n",
    "def build_snn_w_adam(input_dim, learning_rate = 1e-3, beta_1 = 0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, \n",
    "                     metrics=METRICS, output_bias=None):\n",
    "    # initialize output bias if specified\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "#     model.add(layers.Dense(16, activation='relu'))\n",
    "#     model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate, \n",
    "                                                beta_1=beta_1,\n",
    "                                                beta_2=beta_2,\n",
    "                                                epsilon=epsilon,\n",
    "                                                amsgrad=amsgrad), #'adam',\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_snn_w_sgd(input_dim, learning_rate = 0.01, momentum=0.01, nesterov=False, metrics=METRICS, output_bias=None):\n",
    "    # initialize output bias if specified\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.SGD(\n",
    "                                        learning_rate=learning_rate, \n",
    "                                        momentum=momentum, \n",
    "                                        nesterov=nesterov, \n",
    "                                        name=\"SGD\"),\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_lr_w_sgd(input_dim, learning_rate = 0.01, momentum=0.01, nesterov=False, metrics=METRICS, output_bias=None):\n",
    "    # initialize output bias if specified\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(1, activation='sigmoid', input_dim=input_dim, bias_initializer=output_bias))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.SGD(\n",
    "                                        learning_rate=learning_rate, \n",
    "                                        momentum=momentum, \n",
    "                                        nesterov=nesterov, \n",
    "                                        name=\"SGD\"),\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=metrics)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Find correct initial bias, checkpoint the initial weights and confirm whether \n",
    "# the bias fix helps or not\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correct initial bias\n",
    "\n",
    "initial_bias = np.log([pos/neg]) # pos and neg are calculated previously: \n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and review the structure\n",
    "model_b = build_lr_w_sgd(input_dim=x_train_encoded.shape[1])\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model with train dataset\n",
    "\n",
    "model_b.predict(x=x_train_encoded, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate previous model\n",
    "\n",
    "results = model_b.evaluate(x=x_train_encoded, y=y_train, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with initial_bias\n",
    "\n",
    "model_i = build_lr_w_sgd(input_dim=x_train_encoded.shape[1], output_bias=initial_bias)\n",
    "model_i.predict(x=x_train_encoded, steps=10)\n",
    "results = model_i.evaluate(x=x_train_encoded, y=y_train, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial_weights\n",
    "\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model_i.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models with and without initial bias\n",
    "\n",
    "model_v = build_lr_w_sgd(input_dim=x_train_encoded.shape[1])\n",
    "model_v.load_weights(initial_weights)\n",
    "model_v.layers[-1].bias.assign([0.0])\n",
    "zero_bias_history = model_v.fit(\n",
    "    x=x_train_encoded, \n",
    "    y=y_train,\n",
    "    validation_data=(x_val_encoded, y_val), \n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    verbose=0)\n",
    "\n",
    "model_v = build_lr_w_sgd(input_dim=x_train_encoded.shape[1])\n",
    "model_v.load_weights(initial_weights)\n",
    "careful_bias_history = model_v.fit(\n",
    "    x=x_train_encoded, \n",
    "    y=y_train,\n",
    "    validation_data=(x_val_encoded, y_val), \n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defint plot_loss\n",
    "\n",
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the bias fix with plot_loss graphs\n",
    "\n",
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.7,1])\n",
    "#             plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot_confusion_matrix function\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('No Hospitalization Correctly Detected (True Negatives): ', cm[0][0])\n",
    "    print('Hospitalization Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "    print('No Hospitalization Missed (False Negatives): ', cm[1][0])\n",
    "    print('Hospitalization Detected (True Positives): ', cm[1][1])\n",
    "    print('Total Hospitalization: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot_receiver_operating_characteristic function\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "#     plt.xlim([-0.5,20])\n",
    "#     plt.ylim([80,100.5])\n",
    "    plt.xlim([-0.5,100.5])\n",
    "    plt.ylim([40,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Start training by specifying class (e.g., label, y) weights\n",
    "# \n",
    "# Note: we are trying to make the model to pay more attention to under-represented data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv logger\n",
    "csv_logger = CSVLogger(TARGET + \"hospitalization_logistic_regression_8_train_history.csv\", append=False, separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model_weighted = build_lr_w_sgd(learning_rate=LEARNING_RATE, momentum=MOMENTUM, nesterov=True, input_dim=x_train_encoded.shape[1])\n",
    "model_weighted.load_weights(initial_weights)\n",
    "\n",
    "weighted_history = model_weighted.fit(\n",
    "    x=x_train_encoded, \n",
    "    y=y_train,\n",
    "    validation_data=(x_val_encoded, y_val), \n",
    "    class_weight=class_weight, # class weight added\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(weighted_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Evaluate metrics with model_weighted\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_weighted = model_weighted.predict(x=x_train_encoded, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = model_weighted.predict(x=x_test_encoded, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "\n",
    "weighted_results = model_weighted.evaluate(x=x_test_encoded, y=y_test,\n",
    "                                          batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "for name, value in zip(model_weighted.metrics_names, weighted_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(y_test, test_predictions_weighted)\n",
    "\n",
    "#\n",
    "# [RESULTS]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the roc\n",
    "\n",
    "plot_roc(\"Train Weighted\", y_train, train_predictions_weighted, color=colors[1])\n",
    "plot_roc(\"Test Weighted\", y_test, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and bias\n",
    "#\n",
    "\n",
    "print(\"====== weights and bias ==========\")\n",
    "w = np.asarray(model_weighted.get_weights()[0])\n",
    "b = np.asarray(model_weighted.get_weights()[1]) \n",
    "print(w, b)\n",
    "print(\"====== e^weights odds ratio =========\")\n",
    "print(np.exp(w).flatten())\n",
    "print(\"====== e^bias odds ratio =========\")\n",
    "print(np.exp(b).flatten())\n",
    "\n",
    "# save it\n",
    "coeff_dic = {}\n",
    "for name, value in zip(predictor, w.flatten()):\n",
    "    coeff_dic[name] = [value]\n",
    "\n",
    "coeff_dic[\"bias\"] = b.flatten()\n",
    "\n",
    "coeff_df = pd.DataFrame(coeff_dic, columns=coeff_dic.keys())\n",
    "coeff_df.to_csv(TARGET + \"lr_hospital_coefficients.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_json = model_weighted.to_json()\n",
    "with open(TARGET + \"lr_hospital.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model_weighted.save_weights(TARGET + \"lr_hospital.h5\")\n",
    "\n",
    "# save StandardScaler\n",
    "pickle.dump(scaler_age, open(TARGET + \"lr_hospital_age_scaler.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test results in csv file\n",
    "\n",
    "metrics_dic = {}\n",
    "for name, value in zip(model_weighted.metrics_names, weighted_results):\n",
    "    metrics_dic[name] = [value]\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_dic, columns=model_weighted.metrics_names)\n",
    "metrics_df.to_csv(TARGET + \"hospitalization_logistic_regression_8_test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train and test rocs in csv file\n",
    "\n",
    "fp, tp, threshold = sklearn.metrics.roc_curve(y_train, train_predictions_weighted)\n",
    "roc_dic = {\n",
    "    \"threshold\": threshold,\n",
    "    \"fp\": fp,\n",
    "    \"tp\": tp\n",
    "}\n",
    "roc_df = pd.DataFrame(roc_dic, columns=[\"threshold\", \"fp\", \"tp\"])\n",
    "roc_df.sort_values([\"threshold\"]).to_csv(TARGET + \"hospitalization_logistic_regression_8_train_roc.csv\", index=False)\n",
    "\n",
    "fp, tp, threshold = sklearn.metrics.roc_curve(y_test, test_predictions_weighted)\n",
    "roc_dic = {\n",
    "    \"threshold\": threshold,\n",
    "    \"fp\": fp,\n",
    "    \"tp\": tp\n",
    "}\n",
    "roc_df = pd.DataFrame(roc_dic, columns=[\"threshold\", \"fp\", \"tp\"])\n",
    "roc_df.sort_values([\"threshold\"]).to_csv(TARGET + \"hospitalization_logistic_regression_8_test_roc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
