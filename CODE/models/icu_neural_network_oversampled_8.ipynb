{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#\n",
    "# Perform standart neural network model analysis using sklearn and keras (tensorflow) frameworks\n",
    "# and perform model analysis \n",
    "# \n",
    "# Note: This notebook define the model and use oversampled data set to help the model learn from \n",
    "#       the imbalanced data\n",
    "#\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Set parameters for SNN\n",
    "#\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 300 \n",
    "LEARNING_RATE = 0.001  # ADAM default value: 1e-3\n",
    "\n",
    "FILENAME = \"../visualization/data/icu_cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical \n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\n",
    "\n",
    "import time\n",
    "from time import time\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color for graph\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Data preprocessing\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "\n",
    "file = FILENAME\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify label and predictors\n",
    "\n",
    "label = ['icu']\n",
    "\n",
    "# make sure to place a \"age\" column in the first element of the predictor list\n",
    "# predictor = ['age', 'pneumonia', 'diabetes', 'renal_chronic']\n",
    "predictor = [\n",
    "    'age',\n",
    "    'sex',\n",
    "#     'intubed',\n",
    "    'pneumonia',\n",
    "#     'pregnancy',\n",
    "    'diabetes',\n",
    "    'copd',\n",
    "    'asthma',\n",
    "    'inmsupr',\n",
    "    'hypertension',\n",
    "    'other_disease',\n",
    "    'cardiovascular',\n",
    "    'obesity',\n",
    "    'renal_chronic',\n",
    "    'tobacco'\n",
    "]\n",
    "\n",
    "label, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the dataset\n",
    "\n",
    "print(\"Dataframe shape: {}\".format(df[predictor].shape))\n",
    "df[predictor][0:] # only 5 rows. You can also use either df[predictor].head() or df[predictor].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review imbalanced class column (label)\n",
    "\n",
    "neg, pos = np.bincount(df['icu'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling\n",
    "\n",
    "under_df = df.copy()\n",
    "train_df = under_df.sample(frac=0.70, random_state=1337)\n",
    "interm_df = under_df.drop(train_df.index)\n",
    "val_df = interm_df.sample(frac=0.50, random_state=1337) # half of 30%\n",
    "test_df = interm_df.drop(val_df.index)\n",
    "print(\n",
    "    \"Using %d samples for training, %d for validation and %d for testing\"\n",
    "    % (len(train_df), len(val_df), len(test_df))\n",
    ")\n",
    "\n",
    "df_train_neg = train_df[train_df['icu'] == 0]\n",
    "df_train_pos = train_df[train_df['icu'] == 1]\n",
    "print(\"neg: {}, pos: {}\".format(df_train_neg.shape, df_train_pos.shape))\n",
    "\n",
    "repeat = int((df_train_pos.shape[0]/df_train_neg.shape[0])*100)\n",
    "print(\"pos / neg ~= {}\".format(repeat))\n",
    "\n",
    "df_train_pos_oversampled = df_train_pos.copy().sample(frac=1)\n",
    "for i in range(repeat):\n",
    "    df_train_pos_oversampled = df_train_pos_oversampled.append(df_train_pos.sample(frac=1))\n",
    "print(\"df_train_neg: {}, f_train_pos_oversampled: {}\".format(df_train_neg.shape, df_train_pos_oversampled.shape))\n",
    "\n",
    "df_train_balanced = df_train_neg.append(df_train_pos_oversampled)\n",
    "print(\"df_train_balanced: {}\".format(df_train_balanced.shape))\n",
    "\n",
    "train_df = ((df_train_balanced.sample(frac=1)).sample(frac=1)).sample(frac=1)\n",
    "print(\"oversampled train_df: {}\".format(train_df.shape))\n",
    "\n",
    "print(\"label mean after oversampling: {}\".format(train_df[label].values.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numpy n-dimentional array (tensor) from the dataset (pandas' dataframe object)\n",
    "\n",
    "x_train = train_df[predictor].values\n",
    "y_train = train_df[label].values\n",
    "\n",
    "x_test = test_df[predictor].values\n",
    "y_test = test_df[label].values\n",
    "\n",
    "x_val = val_df[predictor].values\n",
    "y_val = val_df[label].values\n",
    "\n",
    "print(\"train shape: [features={}, label={}] \\ntest shape: [features={}, label={}] \\nvalidation shape: [features={}, label={}]\".format(x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_val.shape, y_val.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset using one-hot encoder for categorical columns and standardscaler (mean: 0, std: 1) for numberic columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess train_data\n",
    "\n",
    "scaler_age = StandardScaler().fit(x_train[0:, 0:1])\n",
    "\n",
    "x_train_age = scaler_age.transform(x_train[0:, 0:1])\n",
    "x_train_remaining = encoder.fit_transform(x_train[0:, 1:])\n",
    "# x_train_remaining = x_train[0:, 1:] # 0 or 1\n",
    "\n",
    "x_train_encoded = np.concatenate((x_train_age, x_train_remaining), axis=1)\n",
    "\n",
    "print(\"age column mean: {}, std: {}\".format(scaler_age.mean_, scaler_age.scale_))\n",
    "print(\"x_train encoded shape: {}\".format(x_train_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test_data\n",
    "\n",
    "x_test_age = scaler_age.transform(x_test[0:, 0:1])\n",
    "x_test_remaining = encoder.fit_transform(x_test[0:, 1:])\n",
    "# x_test_remaining = x_test[0:, 1:]\n",
    "\n",
    "x_test_encoded = np.concatenate((x_test_age, x_test_remaining), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess val_data\n",
    "\n",
    "x_val_age = scaler_age.transform(x_val[0:, 0:1])\n",
    "x_val_remaining = encoder.fit_transform(x_val[0:, 1:])\n",
    "# x_val_remaining = x_val[0:, 1:]\n",
    "\n",
    "x_val_encoded = np.concatenate((x_val_age, x_val_remaining), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Build ml models\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for ml models\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define various ml models e.g., standard neural network and logistic regression\n",
    "\n",
    "def build_snn_w_adam(input_dim, learning_rate = 1e-3, beta_1 = 0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, \n",
    "                     metrics=METRICS, output_bias=None):\n",
    "    # initialize output bias if specified\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "#     model.add(layers.Dense(16, activation='relu'))\n",
    "#     model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate, \n",
    "                                                beta_1=beta_1,\n",
    "                                                beta_2=beta_2,\n",
    "                                                epsilon=epsilon,\n",
    "                                                amsgrad=amsgrad), #'adam',\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_snn_w_sgd(input_dim, learning_rate = 0.01, momentum=0.01, nesterov=False, metrics=METRICS, output_bias=None):\n",
    "    # initialize output bias if specified\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.SGD(\n",
    "                                        learning_rate=learning_rate, \n",
    "                                        momentum=momentum, \n",
    "                                        nesterov=nesterov, \n",
    "                                        name=\"SGD\"),\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_lr_w_sgd(input_dim, learning_rate = 0.01, momentum=0.01, nesterov=False, metrics=METRICS, output_bias=None):\n",
    "    # initialize output bias if specified\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(1, activation='sigmoid', input_dim=input_dim, bias_initializer=output_bias))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.SGD(\n",
    "                                        learning_rate=learning_rate, \n",
    "                                        momentum=momentum, \n",
    "                                        nesterov=nesterov, \n",
    "                                        name=\"SGD\"),\n",
    "                    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                    metrics=metrics)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Find correct initial bias, checkpoint the initial weights and confirm whether \n",
    "# the bias fix helps or not\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correct initial bias\n",
    "\n",
    "initial_bias = np.log([pos/neg]) # pos and neg are calculated previously: \n",
    "print(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and review the structure\n",
    "\n",
    "model_b = build_snn_w_adam(input_dim=x_train_encoded.shape[1])\n",
    "model_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model with train dataset\n",
    "\n",
    "model_b.predict(x=x_train_encoded, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate previous model\n",
    "\n",
    "results = model_b.evaluate(x=x_train_encoded, y=y_train, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with initial_bias\n",
    "\n",
    "model_i = build_snn_w_adam(input_dim=x_train_encoded.shape[1], output_bias=initial_bias)\n",
    "model_i.predict(x=x_train_encoded, steps=10)\n",
    "results = model_i.evaluate(x=x_train_encoded, y=y_train, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial_weights\n",
    "\n",
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model_i.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models with and without initial bias\n",
    "\n",
    "model_v = build_snn_w_adam(input_dim=x_train_encoded.shape[1])\n",
    "model_v.load_weights(initial_weights)\n",
    "model_v.layers[-1].bias.assign([0.0])\n",
    "zero_bias_history = model_v.fit(\n",
    "    x=x_train_encoded, \n",
    "    y=y_train,\n",
    "    validation_data=(x_val_encoded, y_val), \n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    verbose=0)\n",
    "\n",
    "model_v = build_snn_w_adam(input_dim=x_train_encoded.shape[1])\n",
    "model_v.load_weights(initial_weights)\n",
    "careful_bias_history = model_v.fit(\n",
    "    x=x_train_encoded, \n",
    "    y=y_train,\n",
    "    validation_data=(x_val_encoded, y_val), \n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defint plot_loss\n",
    "\n",
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the bias fix with plot_loss graphs\n",
    "\n",
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Plot functions\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.7,1])\n",
    "#             plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot_receiver_operating_characteristic function\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "#     plt.xlim([-0.5,20])\n",
    "#     plt.ylim([80,100.5])\n",
    "    plt.xlim([-0.5,100.5])\n",
    "    plt.ylim([40,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Start training by specifying class (e.g., label, y) weights\n",
    "# \n",
    "# Note: we are trying to make the model to pay more attention to under-represented data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "model = build_snn_w_adam(learning_rate=LEARNING_RATE, input_dim=x_train_encoded.shape[1])\n",
    "model.load_weights(initial_weights)\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train_encoded, \n",
    "    y=y_train,\n",
    "    validation_data=(x_val_encoded, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot_confusion_matrix function\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('No ICU Correctly Detected (True Negatives): ', cm[0][0])\n",
    "    print('ICU Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "    print('No ICU Missed (False Negatives): ', cm[1][0])\n",
    "    print('ICU Detected (True Positives): ', cm[1][1])\n",
    "    print('Total ICU: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#\n",
    "# Evaluate metrics with model_weighted\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(x=x_train_encoded, batch_size=BATCH_SIZE)\n",
    "test_predictions = model.predict(x=x_test_encoded, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "\n",
    "results = model.evaluate(x=x_test_encoded, y=y_test,\n",
    "                                          batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(y_test, test_predictions)\n",
    "\n",
    "#\n",
    "# [RESULTS]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the roc\n",
    "\n",
    "plot_roc(\"Train Weighted\", y_train, train_predictions, color=colors[1])\n",
    "plot_roc(\"Test Weighted\", y_test, test_predictions, color=colors[1], linestyle='--')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
